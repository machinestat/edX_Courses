---
title: "Introduction to Linear Models"
author: "Shu"
date: "Wednesday, July 15, 2015"
output: pdf_document
---

## Introduction

We are going to describe three examples from the life sciences. One from physics, one related to genetics, and one from a mouse experiment. They are very different yet we end up using the same statistical technique: fitting linear models. 
```{r}
# install_github('rafalib','ririzarr')
library(rafalib)
## Loading required package: RColorBrewer
mypar2()
```

## Objects falling

Imagine you are Galileo back in the 16th century trying to describe the velocity of an objects falling. An assistant climbs the Tower of Pizza and drops a ball while several others record the position at different times. Let’s simulate some data using the equations we know today and adding some measurement error:
```{r}
set.seed(1)
g <- 9.8 # meters per second
n <- 25
tt <- seq(0, 3.4, len = n) # time in seconds, t is a base function
d <- 56.67 - 0.5*g*tt^2 + rnorm(n, sd = 1)

# The assistants hand the data to Galileo and this is what he sees:
plot(tt, d, ylab = "Distance in meters",
     xlab = "Time in seconds")
```

The model of the data is:

$$ Y_{i} = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \varepsilon$$ $$i=1,\dots,n$$

With $Y_i$ representing location, $x_i$ representing the time, and $\varepsilon$ accounts for measurement error. This is a linear model becuase it is a linear combination of known quantities (th $x$ s) referred to as predictors or covariates and unknown parameters (the $\beta$ s).

## Father son’s heights

Paired height data from father and sons
```{r}
library(UsingR)
x=father.son$fheight
y=father.son$sheight
plot(x, y, xlab = "Father's height", ylab = "Son's height")
```

The son’s height does seem to increase linearly with father’s height. In this case a model that describes the data is as follows:

$$Y_i = \beta_0 + \beta_1 x_i + \varepsilon,  i = 1, \dots, N $$

This is also a linear model. Here $x_i$ and $Y_i$ the father and son heights respectively for the $i$-th pair and $\varepsilon$ a term to account for the extra variability. Here we think of the father's height as the predictor and being fixed (not random) so we use lower case. Note that measurement error alone can't explain all the variability seen in $\varepsilon$. Note that this makes sense as there are other variables not in the model, for example, mother's height and environmentalism factors.

## Random samples from multiple populations

Here we read-in mouse body weight data from mice that were fed two different diets, high fat and control (chow). We have a random sample of 12 mice for each. We are interested in determining if the diet has an effect on weight. Here is the data

```{r}
# library(devtools)
# install_github("genomicsclass/dagdata")
dir <- system.file(package="dagdata")
filename <- file.path(dir,"extdata/femaleMiceWeights.csv")
dat <- read.csv(filename)
mypar2(1,1)
stripchart(Bodyweight~Diet,data=dat,vertical=TRUE,method="jitter",pch=1,main="Mice weights")
```

We want to estimate the difference in average weight between populations. We showed how to do this using t-tests and confidence intervals based on the difference in sample averages. We can obtain the same exact results using a linear model:

$$ Y_i = \beta_0 + \beta_1 x_{i} + \varepsilon_i $$

with $\beta_0$ the chow diet average weight, $\beta_1$ the difference between averages, $x_i = 1$ when mouse $i$ gets the high fat (hf) diet, $x_i = 0$ when it gets the chow diet, and $\varepsilon_i$ explains the differences between mice of same population.

## Linear model in general

We have seen three very different examples in which linear models can be used. A general model that encompasses all of the above examples is the following:

$$ Y_i = \beta_0 + \beta_1 x_{i,1} + \beta_2 x_{i,2} + \dots + \beta_2 x_{i,p} + \varepsilon_i, i=1,\dots,n $$

$$ Y_i = \beta_0 + \sum_{j=1}^p \beta_j x_{i,j} + \varepsilon_i, i=1,\dots,n $$

Note that we have a general number of predictors $p$. Matrix algebra provides a compact language and mathematical framework to compute and make derivations with any linear models that first into the above framework.

## Estimating parameters

For the models above to be useful we have to estimate the unknown $\beta$ s. In the first example, we want to describe a physical process for which we can't have unknown parameters. In the second example we better understand inheritence by estimating how much, on average, father height affects the son's. In the final example we want to determine if their is infact a difference: if $\beta_1 \neq 0$.

The standard approach in science is to find the values that minimize the distance of the fitted model to the data. The following is called the least squares (LS) equation and we will see it often in this chapter:

$$ \sum_{i=1}^n { Y_i - (\beta_0 + \sum_{j=1}^p \beta_j x_{i,j}}^2 $$

Once we find the minimum, we will call the values the least squares estimates (LSE) and denote them with $\hat{\beta}$. The quantity obtained when evaluating the least square equation at the estimates is called the residual sum of squares (RSS). Note that because all these quantities depend on $Y$, they are random variables. The $\hat{\beta}$ s are random variables and we will eventually perform inference on them.

## Falling object example revisited

The equation for the falling object is:

$$d = h_0 + v_0 t - 0.5 \times 9.8 t^2$$

with $h_0$ and $v_0$ the starting height and velocity respectively. The data we simulated above followed this equation and added measurement error to simulate n observations for dropping the ball $(v_0=0)$ from the tower of Pisa $(h_0=56.67)$. This is why we used this code to simulate data:

```{r}
g <- 9.8 ## per second
n <- 25
tt <- seq(0, 3.4, len = n) ## time in secs, t is a base function
f <- 56.67 - 0.5*g*tt^2
y <- f + rnorm(n, sd = 1)
```

Here is what the data looks like with the solid line representing the true trajectory:

```{r}
plot(tt,y,ylab="Distance in meters",xlab="Time in seconds")
lines(tt,f,col=2)
```

The data does suggest it is a parabola so we model as such:

$$ Y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \varepsilon, i=1,\dots,n $$

## The lm function

In R we can fit this model by simply using the lm function. We will describe this function in detail later, but here is a preview

```{r}
tt2 <- tt^2
fit <- lm(y ~ tt + tt2)
summary(fit)$coef
```

It gives us the LSE as well as standard errors and p-values.

## The LSE

Let's write a function that computes the RSS for any vector $\beta$

```{r}
rss <- function(Beta0, Beta1, Beta2){
    r <- y - (Beta0 + Beta1*tt + Beta2*tt^2)
    return(sum(r^2))
}
```

So for any three dimensional vector we get an RSS. Here is a plot of the RSS as a function of $\beta_2$ when we keep the other two fixed:

```{r}
Beta2s<- seq(-10,0,len=100)
plot(Beta2s,sapply(Beta2s,rss,Beta0=55,Beta1=0),
     ylab="RSS",xlab="Beta2",type="l")
##Let's add another curve fixing another pair:
Beta2s<- seq(-10,0,len=100)
lines(Beta2s,sapply(Beta2s,rss,Beta0=65,Beta1=0),col=2)
```

Trial and error here is not going to work. Instead we can use calculus: take the partial derivatives, set them to 0 and solve. But note that if we have many parameters, these equations can get rather complex. Linear algebra provides a compact and general way of solving this problem.

## Qustions

QUESTION 1.1.1: 

```{r}
library(UsingR)

?father.son

mean(father.son$sheight)
```

QUESTION 1.1.2 

One of the defining features of regression is that we stratify one variable based on others. In Statistics we use the verb "condition". For example, the linear model for son and father heights answers the question how tall do I expect a son to be if I condition on his father being x inches. The regression line answers this question for any x.

Using the father.son dataset described above, we want to know the expected height of sons if we condition on the father being 71 inches. Create a list of son height's for sons that have fathers with height of 71 inches (round to the nearest inch).

What is the mean of the son heights for fathers that have a height of 71 inches (don't round off your answer)? 

```{r}
round(father.son[round(father.son$fheight) == 71, 2])

mean(father.son[round(father.son$fheight) == 71, 2])

```

## Matrix Notation

### The language of linear models

Linear algebra notation actually simplifies the mathematical descriptions and manipulations of linear models as well are coding in R. The main point  is to show how we can write the models above using matrix notation and then explain how this is useful for solving the least squares equation. 

### Motivation

Linear algebra was created by mathematicians to solve systems of linear equations such as this:

$$ \begin{aligned} a + b + c &= 6\\  
                 3a - 2b + c &= 2\\ 
                 2a + b - c &= 1 \end{aligned} $$

It provides very useful machinery to solve these problems generally.

$$ \begin{pmatrix} 1&1&1\ 3&-2&1\ 2&1&-1 \end{pmatrix} \begin{pmatrix} a\ b\ c \end{pmatrix} = \begin{pmatrix} 6\ 2\ 1 \end{pmatrix} \implies \begin{pmatrix} a\ b\ c \end{pmatrix} = \begin{pmatrix} 1&1&1\ 3&-2&1\ 2&1&-1 \end{pmatrix}^{-1} \begin{pmatrix} 6\ 2\ 1 \end{pmatrix} $$

### Vectors, Matrices and Scalars

In the examples above the random variables associated with the data were represented by $ Y_1, \dots, Y_n $. We can think of this as a vector. In fact, in R we are already doing this:

```{r}
library(UsingR)
y=father.son$fheight
head(y)
```
In math we can also use just one symbol and we usually use bold to distinguish it from the individual entries:  
$$ \mathbf{Y} = \begin{pmatrix} Y_1\\ Y_2\\ \vdots\\ Y_N \end{pmatrix} $$

Default representation of data vectors have dimension $N×1$ as opposed to $1×N$ .

Similarly, use math notation to represent the covariates or predictors. In the case of the two, with the second one just being the square of the first.

$$ \mathbf{X}1 = \begin{pmatrix} x{1,1}\ \vdots\ x_{N,1} \end{pmatrix} \mbox{ and } \mathbf{X}2 = \begin{pmatrix} x{1,2}\ \vdots\ x_{N,2} \end{pmatrix} $$

it is convenient to representing these in matrices:

$$ \mathbf{X} = [ \mathbf{X}1 \mathbf{X}_2 ] = \begin{pmatrix} x{1,1}&x_{1,2}\ \vdots\ x_{N,1}&x_{N,2} \end{pmatrix} $$

This matrix has dimension $N×2$ . We can create this matrix in R this way

```{r}
n <- 25
tt <- seq(0,3.4,len=n) ##time in secs, t is a base function
X <- cbind(x1=tt,x2=tt^2)
head(X)
dim(X)
```

se this notation to denote an arbitrary number of covariates with the following $N×p$ matrix:

$$ \mathbf{X} = \begin{pmatrix} x_{1,1}&\dots & x_{1,p} \ x_{2,1}&\dots & x_{2,p} \ & \vdots & \ x_{N,1}&\dots & x_{N,p} \end{pmatrix} $$

```{r}
N <- 100; p <- 5
X <- matrix(1:(N*p),N,p)
head(X)

# Note that the columns are filled by column. Use byrow=TRUE argument to change that:

N <- 100; p <- 5
X <- matrix(1:(N*p),N,p,byrow=TRUE)
head(X)
```

### QUESTIONS

1.2.1
In R we have vectors and matrices. You can create your own vectors with the function c.

c(1,5,3,4)

They are also the output of many functions such as

rnorm(10)

You can turn vectors into matrices using functions such as rbind, cbind or matrix.

Create the matrix from the vector 1:1000 like this:

X = matrix(1:1000,100,10)

What is the entry in row 25, column 3 ?

```{r}
X = matrix(1:1000,100,10)
X[25, 3]
```

1.2.2

Using the function cbind, create a 10 x 5 matrix with first column

x=1:10

Then columns 2*x, 3*x, 4*x and 5*x in columns 2 through 5.

What is the sum of the elements of the 7th row?

```{r}
x = 1:10

y <- cbind(x, 2*x, 3*x, 4*x, 5*x)

sum(y[7, ])

```

## Matrix Operations

### Multiplying by a scalar

If $a$ is scalar and `X` is a matrix then:

$$ a \mathbf{X} = \begin{pmatrix} a x_{1,1} & \dots & a x_{1,p}\ & \vdots & \ a x_{N,1} & \dots & a x_{N,p} \end{pmatrix} $$

```{r}
X <- matrix(1:12,4,3)
print(X)
a <- 2
print(a*X)    
```

### The Transpose

The transpose is an operation that simply changes columns to rows. We use either a $T$ or ′ to denote transpose. Here is the technical definition. If $X$ is as we defined it above, here is the transpose which will be $p×N$:

$$ \mathbf{X} = \begin{pmatrix} x_{1,1}&\dots & x_{1,p} \ x_{2,1}&\dots & x_{2,p} \ & \vdots & \ x_{N,1}&\dots & x_{N,p} \ \end{pmatrix} \implies \mathbf{X}^\top = \begin{pmatrix} x_{1,1}&\dots & x_{p,1} \ x_{1,2}&\dots & x_{p,2} \ & \vdots & \ x_{1,N}&\dots & x_{p,N} \ \end{pmatrix} $$

```{r}
X <- matrix(1:12,4,3)
X
t(X)
```

### Matrix multiplication

```{r}
X  <- matrix(c(1,3,2,1,-2,1,1,1,-1),3,3)
abc <- c(3,2,1) ## use as an example
rbind( sum(X[1,]*abc), sum(X[2,]*abc), sum(X[3,]%*%abc))

## use the %*% to perform the matrix multiplication 
X%*%abc
```

### The identity matrix

form an identity matrix in R:

```{r}
n <- 5 ##pick dimensions
diag(n)
```

### The inverse

The inverse of matrix of $X$, denoted with $X^{-1}$ has the property that when multiplied gives the identity $X^{-1}X=I$.

```{r}
X <- matrix(c(1,3,2,1,-2,1,1,1,-1),3,3)
y <- matrix(c(6,2,1),3,1)
solve(X)%*%y ##equivalent to solve(X,y)
```
## Linear Algebra Examples

write linear models with matrix algebra notation and solve the least squares problem.

### The average

To compute the sample average and variance of our data we use these formulas $\bar{Y}=\frac{1}{N} Y_i$ and $\mbox{var}(Y)=\frac{1}{N} \sum_{i=1}^N (Y_i - \bar{Y})^2$. We can represent these with matrix multiplication. First define this $N \times 1$ matrix made just of 1s

$$ A=\begin{pmatrix} 1\ 1\ \vdots\ 1 \end{pmatrix} $$

This implies that

$$ \frac{1}{N} \mathbf{A}^\top Y = \frac{1}{N} \begin{pmatrix}1&1&,\dots&1\end{pmatrix} \begin{pmatrix} Y_1\ Y_2\ \vdots\ Y_N \end{pmatrix}= \frac{1}{N} \sum_{i=1}^N Y_i = \bar{Y} $$

```{r}
library(UsingR)
y <- father.son$sheight
print(mean(y))

N <- length(y)
Y <- matrix(y,N,1)
A <- matrix(1,N,1)
barY=t(A)%*%Y / N

print(barY)

# multiplying the transpose of a matrix with another using crossprod()
barY=crossprod(A,Y) / N
print(barY)
```
